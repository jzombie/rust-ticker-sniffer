File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/benches/my_benchmark.rs
-------------------------------------
#![allow(dead_code, unused_imports, unused_variables)]

#[path = "../test_utils/lib.rs"]
mod test_utils;
use test_utils::constants::TEST_SYMBOLS_CSV_PATH;
use test_utils::load_company_symbol_list_from_file;

use criterion::{black_box, criterion_group, criterion_main, Criterion};

use ticker_sniffer::{extract_tickers_from_text, ResultBiasAdjuster};

fn benchmark_extract_tickers(c: &mut Criterion) {
    let symbols_map =
        load_company_symbol_list_from_file(TEST_SYMBOLS_CSV_PATH).expect("Failed to load symbols from CSV");

    let text = "AAPL is performing well, but MSFT is also a strong contender. There are also Amazon is another company.";

    c.bench_function("extract_tickers", |b| {
        b.iter(|| extract_tickers_from_text(black_box(text), black_box(&symbols_map)))
    });
}

criterion_group!(benches, benchmark_extract_tickers);
criterion_main!(benches);

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/bin_utils/lib.rs
-------------------------------------
pub mod suppress_output;

pub use suppress_output::suppress_output;

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/bin_utils/suppress_output.rs
-------------------------------------
use std::fs::File;
use std::io;
use std::os::unix::io::AsRawFd;

pub fn suppress_output<F, T>(f: F, should_suppress: bool) -> T
where
    F: FnOnce() -> T,
{
    if should_suppress {
        let dev_null = File::open("/dev/null").expect("Failed to open /dev/null");
        let null_fd = dev_null.as_raw_fd();

        // Backup stdout and stderr using `dup`
        let stdout_backup = unsafe { libc::dup(io::stdout().as_raw_fd()) };
        let stderr_backup = unsafe { libc::dup(io::stderr().as_raw_fd()) };

        if stdout_backup < 0 || stderr_backup < 0 {
            panic!("Failed to backup stdout or stderr");
        }

        // Redirect stdout and stderr to /dev/null
        unsafe {
            libc::dup2(null_fd, io::stdout().as_raw_fd());
            libc::dup2(null_fd, io::stderr().as_raw_fd());
        }

        let result = f(); // Run the closure

        // Restore original stdout and stderr
        unsafe {
            libc::dup2(stdout_backup, io::stdout().as_raw_fd());
            libc::dup2(stderr_backup, io::stderr().as_raw_fd());
            libc::close(stdout_backup); // Close the backup descriptors
            libc::close(stderr_backup);
        }

        result
    } else {
        f() // Execute without suppressing output
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/bin/proto_n_gram.rs
-------------------------------------
#![allow(dead_code, unused_imports, unused_variables)]

#[path = "../../test_utils/lib.rs"]
mod test_utils;
use test_utils::constants::TEST_SYMBOLS_CSV_PATH;
use test_utils::load_company_symbol_list_from_file;
use ticker_sniffer::extract_tickers_from_text;

use log::{debug, error, info, log_enabled, Level};

use std::collections::HashMap;

fn main() {
    env_logger::init();

    let company_symbols_list = load_company_symbol_list_from_file(TEST_SYMBOLS_CSV_PATH)
        .expect("Failed to load symbols from CSV");

    // let mut matches = HashSet::new();

    // TODO: Commit to tokenizer tests; expect: Tokens: ["WELL", "IPHONE", "DEVELOPMENT", "EBAY", "DEVELOPMENT", "WALMART", "WALMARTS"]
    // let tokens = tokenize(
    //     &"Well okay iPhone turtle develoPment e-Bay  Deve-\nlopment Wal-mart's at it again!",
    // );

    // TODO: Commit to tokenizer tests; expect: Tokens: ["ECOMMERCE", "AMAZONCOM", "AMAZON", "INC", "AMZN", "QUICK", "QUOTEAMZN", "FREE", "REPORT", "DOW", "JONES", "INDUSTRIAL", "AVERAGE", "WALGREENS", "BOOTS", "ALLIANCE", "WBA", "QUICK", "QUOTEWBA", "FREE", "REPORT", "FEB", "THE", "THE", "AMAZON", "DOW"]
    // let tokens = tokenize(
    //     &"E-commerce giant Amazon.com Inc. (AMZN Quick QuoteAMZN - Free Report) joined the blue-chip index, Dow Jones Industrial Average, replacing drugstore operator Walgreens Boots Alliance (WBA Quick QuoteWBA - Free Report) on Feb 26. The reshuffle reflects the ongoing shift in economic power from traditional brick-and-mortar retail to e-commerce and technology-driven companies. The inclusion of Amazon in the Dow marks a significant milestone in the recognition of the e-commerce giant's influence and its role in the broader market.",
    // );

    // let query = "The stuff IT dreams are made of, but you know, Agilent is interesting.";

    // let query = "Ashford";
    // let query = "Power REIT";
    // let query = "REIT Hospitality Apple stuff";
    // let query = "Arbor";
    // let query = "Arbor Realty";
    // let query = "Berkshire Hathaway is not Apple, but owns Apple, of course, which is not Apple Hospitality REIT.";
    // let query = "Apple";

    // let query = "Apple";
    // let query = "Berkshire";

    // TODO: Ensure that queries can't be performed in reverse
    // let query = "Hathaway Berkshire Hospitality Hathaway Berkshire Hathaway Apple Apple Hospitality REIT INC REIT 222";
    // let query = "Apple Apple Hospitality REIT Apple INC";
    // let query = "Apple Apple INC Apple Hospitality REIT";
    // let query = "Berkshire Hathaway Berkshire Hathaway";
    // let query = "Hathaway";
    // let query = "Hospitality Apple";

    // let query = "Apple Walmart Hospitality REIT";
    // let query = "Alphabet";
    // let query = "Amazon";
    // let query = "SPDR Dow Jones Industrial Average";
    // let query = "Dow Jones Industrial, and well, Apple Computers.";
    // let query = "The article mentions Dow Chemical Company as one of the key players in the recycled plastic market, indicating their involvement and potential to benefit from the market growth.";

    // TODO: Locate source
    // TODO: [add test] Figure out why APLE is showing up instead of AAPL
    // TODO: [add test] Check for `EDOW` (First Trust Dow 30 Equal Weight ETF) in the results
    let query = r#"E-commerce giant Amazon.com Inc. (AMZN Quick QuoteAMZN - Free Report) joined the blue-chip index, Dow Jones Industrial Average, replacing drugstore operator Walgreens Boots Alliance (WBA Quick QuoteWBA - Free Report) on Feb 26. The reshuffle reflects the ongoing shift in economic power from traditional brick-and-mortar retail to e-commerce and technology-driven companies. The inclusion of Amazon in the Dow marks a significant milestone in the recognition of the e-commerce giant's influence and its role in the broader market.
        The shift was prompted by Walmart's (WMT Quick QuoteWMT - Free Report) decision to execute a 3-to-1 stock split, which has reduced its stock's weighting in the index. The Dow is a price-weighted index. So, stocks that fetch higher prices are given more weight. Amazon's addition has increased consumer retail exposure within the index, alongside enhancing the representation of various other business sectors that Amazon engages in, including cloud computing, digital streaming and artificial intelligence, among others (read: Walmart Soars on Earnings, Dividend & Vizio Deal: ETFs to Buy).
        Amazon took the 17th position in the index, while Walmart's weighting dropped to 26 from 17. UnitedHealth Group remained the most heavily weighted stock in the index. Amazon's entry into the Dow Jones is not just a symbolic change but a reflection of the evolving priorities and dynamics within the investment world. It signals a broader recognition of the value and impact of technology and e-commerce sectors, encouraging investors to perhaps rethink their investment approaches in light of these trends.

        While the Dow Jones is making new record highs, its performance is lagging behind the S&P and Nasdaq over the past year. The underperformance is due to the lack of exposure in tech stocks and the “Magnificent Seven” companies in particular. The Dow includes two of the Magnificent Seven — Apple (AAPL Quick QuoteAAPL - Free Report) and Microsoft (MSFT Quick QuoteMSFT - Free Report) . Amazon will be the third. As such, the addition of Amazon will help Dow Jones catch up with the S&P 500 gains. The shares of the commerce giant have surged more than 80% over the past year (read: ETFs to Tap on Amazon's Strong Q4 Earnings).

        Given this, investors seeking to tap the potential strength in the Dow Jones trend could consider SPDR Dow Jones Industrial Average ETF (DIA Quick QuoteDIA - Free Report) , iShares Dow Jones U.S. ETF (IYY Quick QuoteIYY - Free Report) , Invesco Dow Jones Industrial Average Dividend ETF (DJD Quick QuoteDJD - Free Report) and First Trust Dow 30 Equal Weight ETF (EDOW Quick QuoteEDOW - Free Report) .

        ETFs to Tap
        SPDR Dow Jones Industrial Average ETF (DIA Quick QuoteDIA - Free Report)
        SPDR Dow Jones Industrial Average ETF is one of the largest and most popular ETFs in the large-cap space, with AUM of $33.1 billion and an average daily volume of 3.8 million shares. It tracks the Dow Jones Industrial Average Index, holding 30 stocks in its basket with each making up for less than 9% share. Financials (21.7%), information technology (19.5%), healthcare (18.5%), consumer discretionary (15.9%) and industrials (14.613.7%) and are the top five sectors (read: Will Dow Jones ETFs Rule in 2024?).

        SPDR Dow Jones Industrial Average ETF charges 16 bps in annual fees and has a Zacks ETF Rank #1 (Strong Buy) with a Medium risk outlook.

        iShares Dow Jones U.S. ETF (IYY Quick QuoteIYY - Free Report)

        iShares Dow Jones U.S. ETF tracks the Dow Jones U.S. Index, holding 1077 stocks in its basket, with none accounting for more than 6.4% of the assets. Information technology takes the largest share at 29%, while financials, healthcare and consumer discretionary round off the next spots with double-digit exposure each.

        iShares Dow Jones U.S. ETF has amassed $1.9 billion in its asset base while trading in an average daily volume of 36,000 shares. It charges 20 bps in annual fees and has a Zacks ETF Rank #3 (Hold) with a Medium risk outlook.

        Invesco Dow Jones Industrial Average Dividend ETF (DJD Quick QuoteDJD - Free Report)

        Invesco Dow Jones Industrial Average Dividend ETF offers exposure to dividend-paying companies included in the Dow Jones Industrial Average by their 12-month dividend yield over the prior 12 months. It holds 27 stocks in its basket, with none accounting for more than 12% of the assets.

        Invesco Dow Jones Industrial Average Dividend ETF has been able to manage assets worth $294.4 million while trading in a volume of 56,000 shares a day on average. It charges 7 bps in annual fees and has a Zacks ETF Rank #3.

        First Trust Dow 30 Equal Weight ETF (EDOW Quick QuoteEDOW - Free Report)

        First Trust Dow 30 Equal Weight ETF offers equal-weight exposure to all the 30 components of the Dow Jones Industrial Average by tracking the Dow Jones Industrial Average Equal Weight Index.

        First Trust Dow 30 Equal Weight ETF has accumulated $249.1 million in its asset base and trades in an average daily volume of 58,000 shares. It charges 50 bps in annual fees.

        Want key ETF info delivered straight to your inbox?
    Zacks’ free Fund Newsletter will brief you on top news and analysis, as well as top-performing ETFs, each week.
        "#;

    let query =
        "A Google search and Apple Hospitality REIT, and then there is the REIT symbol and AAPL, now I will say it again, Apple is a good company, and they team up with Google occasionally though Apple and Google are competitors. A Google search confirms...";

    // let query = "NVDA, GOOG, GOOGL, A, AAPL and AMZN";

    // let query = "First Trust Dow 30 Equal Weight ETF";
    // Note: This is a subset of the previous, filtered to the lines which were are causing EDOW to not be represented in the result
    // let query = r#"

    //         While the Dow Jones is making new record highs, its performance is lagging behind the S&P and Nasdaq over the past year. The underperformance is due to the lack of exposure in tech stocks and the “Magnificent Seven” companies in particular. The Dow includes two of the Magnificent Seven — Apple (AAPL Quick QuoteAAPL - Free Report) and Microsoft (MSFT Quick QuoteMSFT - Free Report) . Amazon will be the third. As such, the addition of Amazon will help Dow Jones catch up with the S&P 500 gains. The shares of the commerce giant have surged more than 80% over the past year (read: ETFs to Tap on Amazon's Strong Q4 Earnings).

    //         Given this, investors seeking to tap the potential strength in the Dow Jones trend could consider SPDR Dow Jones Industrial Average ETF (DIA Quick QuoteDIA - Free Report) , iShares Dow Jones U.S. ETF (IYY Quick QuoteIYY - Free Report) , Invesco Dow Jones Industrial Average Dividend ETF (DJD Quick QuoteDJD - Free Report) and First Trust Dow 30 Equal Weight ETF (EDOW Quick QuoteEDOW - Free Report) .

    //         First Trust Dow 30 Equal Weight ETF (EDOW Quick QuoteEDOW - Free Report)

    //         First Trust Dow 30 Equal Weight ETF offers equal-weight exposure to all the 30 components of the Dow Jones Industrial Average by tracking the Dow Jones Industrial Average Equal Weight Index.

    //     First Trust Dow 30 Equal Weight ETF has accumulated $249.1 million in its asset base and trades in an average daily volume of 58,000 shares. It charges 50 bps in annual fees.

    //         Want key ETF info delivered straight to your inbox?
    //     Zacks’ free Fund Newsletter will brief you on top news and analysis, as well as top-performing ETFs, each week.
    // "#;

    // let query = "Dow Jones Industrial Average";
    // let query = "Berkshire Hathaway";

    // TODO: This includes a lot of repeated "Capital" entries, with only initial 0 window indexes.
    // This type of pattern should be filtered out so it effectively removes them entirely.
    // let query = r#"
    // Amazon
    // has for years counted on millions of third-party sellers to provide the bulk of the inventory that consumers buy. But keeping track of their finances has long been a challenge for outside merchants, particularly smaller mom-and-pop shops.

    // Amazon said Monday that it’s partnering with Intuit
    // to bring the software company’s online accounting tools to its vast network of sellers in mid-2025. Intuit QuickBooks will be available on Amazon Seller Central, the hub sellers use to manage their Amazon businesses, the companies said. Eligible sellers will also have access to loans through QuickBooks Capital.

    // “Together with Intuit, we’re working to equip our selling partners with additional financial tools and access to capital to help them scale efficiently,” Dharmesh Mehta, Amazon’s vice president of worldwide selling partner services, said in the joint release.

    // The companies said sellers will see a real-time view of the financial health of their business, getting a clear picture of profitability, cash flow and tax estimates.

    // While the Intuit integration isn’t expected to go live until the middle of next year, the announcement comes as sellers ramp up their businesses for the holiday season, the busiest time of the year for most retailers.

    // Representatives from both companies declined to provide specific terms of the agreement, including how revenue will be shared.

    // The marketplace is a critical part of Amazon’s retail strategy. In addition to accounting for about 60% of products sold, Amazon generates fees from providing fulfillment and shipping services as well as by offering customer support to sellers and charging them to advertise on the site.

    // In the third quarter, seller services revenue increased 10% to $37.9 billion, accounting for 24% of total revenue, a number that’s steadily increased in recent years. Amazon CEO Andy Jassy said on the earnings call that ”[third-party] demand is still strong and unit volumes are strong.”

    // Amazon shares are up almost 50% this year, climbing to a fresh record Friday, and topping the Nasdaq’s 31% gain for the year. Meanwhile, Intuit has underperformed the broader tech index, with its stock up less than 4% in 2024.

    // Intuit shares dropped 5% on Nov. 19 after The Washington Post reported that President-elect Donald Trump’s government efficiency team is considering creating a free tax-filing app. They fell almost 6% three days later after the company issued a revenue forecast for the current quarter that trailed analysts’ estimates due to some sales being delayed.

    // QuickBooks, which is particularly popular as an all-in-one accounting, expense management and payroll tool for small businesses, has been one of Intuit’s key drivers for growth. The company said in November that its QuickBooks Online Accounting segment expanded by 21% in the latest quarter, while total revenue increased 10% to $3.28 billion.

    // Intuit has been adding generative artificial intelligence tools into QuickBooks and other small business services, such as its Mailchimp email marketing offering, to provide more automated insights for users.

    // “You can imagine, as we look ahead, our goal is to create a done-for-you experience across the entire platform, across Mailchimp and QuickBooks and all of the services,” Intuit CEO Sasan Goodarzi said on the fiscal first-quarter earnings call.

    // Goodarzi said in Monday’s release that the company is bringing its “AI-driven expert platform to help sellers boost their revenue and profitability, save time, and grow with confidence.”
    // "#;

    // TODO: [add test; specifically for company name extractor] Ensure DIA gets extracted using company name
    // let query = r#"
    //     Given this, investors seeking to tap the potential strength in the Dow Jones trend could consider SPDR Dow Jones Industrial Average ETF (DIA Quick QuoteDIA - Free Report) , iShares Dow Jones U.S. ETF (IYY Quick QuoteIYY - Free Report) , Invesco Dow Jones Industrial Average Dividend ETF (DJD Quick QuoteDJD - Free Report) and First Trust Dow 30 Equal Weight ETF (EDOW Quick QuoteEDOW - Free Report) .
    // "#;

    // TODO: [add test] This should only return Apple
    // let query = "Apple Apple Inc Hospitality";

    // let query = "E-commerce giant Amazon.com Inc. (AMZN Quick QuoteAMZN - Free Report) joined the blue-chip index, Dow Jones Industrial Average, replacing drugstore operator Walgreens Boots Alliance (WBA Quick QuoteWBA - Free Report) on Feb 26. The reshuffle reflects the ongoing shift in economic power from traditional brick-and-mortar retail to e-commerce and technology-driven companies. The inclusion of Amazon in the Dow marks a significant milestone in the recognition of the e-commerce giant's influence and its role in the broader market.";

    // let query = r#"
    // Invesco Dow Jones Industrial Average Dividend ETF (DJD Quick QuoteDJD - Free Report)

    //     Invesco Dow Jones Industrial Average Dividend ETF offers exposure to dividend-paying companies included in the Dow Jones Industrial Average by their 12-month dividend yield over the prior 12 months. It holds 27 stocks in its basket, with none accounting for more than 12% of the assets.

    //     Invesco Dow Jones Industrial Average Dividend ETF has been able to manage assets worth $294.4 million while trading in a volume of 56,000 shares a day on average. It charges 7 bps in annual fees and has a Zacks ETF Rank #3.
    // "#;

    //     let query = r#"
    //     Palantir
    // : Cramer praised the software company’s management and said the company’s third quarter, which sent shares surging 20%, was one of the best of the year. He also said he liked the company’s defense business, noting Palantir’s work for the Pentagon.
    // Axon
    // : Axon largely makes equipment for law enforcement, and Cramer suggested it is poised to do well as a GOP trifecta in Washington likely means police will receive more funding. The company also reported better-than-expected quarterly results, and it has new software that includes artificial intelligence, he added.
    // Tesla
    // : Tesla CEO Elon Musk is a close ally of President-elect Donald Trump, and Cramer said his loyalty will probably lead to rewards for the company and its automated vehicle business.

    //     "#;

    // let query = " Invesco Dow Jones Industrial Average Dividend ETF ";

    // let query = r#"SPDR Dow Jones Industrial Average ETF Invesco Dow Jones Industrial Average Dividend ETF iShares Dow Jones U.S. ETF"#;

    // let query = "Apple Walmart Berkshire Hathaway Apple Wal-mart Berkshire Hathaway Dow Jones Industrial Average";

    // let query = "Apple Hospitality REIT";

    // let query = "E-commerce giant Amazon.com Inc. (AMZN Quick QuoteAMZN - Free Report) joined the blue-chip index, Dow Jones Industrial Average, replacing drugstore operator Walgreens Boots Alliance (WBA Quick QuoteWBA - Free Report) on Feb 26. The reshuffle reflects the ongoing shift in economic power from traditional brick-and-mortar retail to e-commerce and technology-driven companies. The inclusion of Amazon in the Dow marks a significant milestone in the recognition of the e-commerce giant's influence and its role in the broader market.";
    // let query = "The DOW Jones Industrial";
    // let query = "Agilent";
    // let query = "NVDA";
    // let query = "Google";
    // let query = "J.P. Morgan Exchange-Traded Fund Trust - JPMorgan BetaBuilders Emerging Markets Equity ETF";
    let query = r#"
    Intel
    on Thursday announced the appointment of two new directors with significant semiconductor manufacturing experience as the company ramps up the search process for ousted CEO Pat Gelsinger’s replacement.

    Former ASML
    CEO Eric Meurice and Microchip Technology
    interim CEO Steve Sanghi will join Intel’s board effective immediately, the company said. Their appointments mean that Intel’s board once again has directors with semiconductor experience, fixing a vacuum left by the departure of Cadence Design Systems Chairman Lip-Bu Tan in August.

    Intel declined to comment on what committees the two new directors would join and the nature of the search process. 

    Intel’s search for new directors predated Gelsinger’s firing, according to people familiar with the matter. The company’s board had been interviewing semiconductor executives for several weeks, said the people, who requested anonymity to discuss confidential information freely.

    Meurice ran ASML, which manufactures some of the most advanced chipmaking machines, for eight years. ASML’s share price quintupled during his tenure, Intel said.

    Sanghi rejoined Microchip as interim CEO in November, after serving as CEO from 1991 to 2016. Sanghi was previously an executive at Intel.

    “Eric and Steve are highly respected leaders in the semiconductor industry whose deep technical expertise, executive experience and operational rigor make them great additions to the Intel board,” interim Executive Chairman Frank Yeary said.

    Sanghi and Meurice join Intel’s board at a critical juncture. The company ousted Gelsinger over the weekend and has since been assembling a short list of replacements with the help of an executive search firm. Intel’s market cap sits firmly below $100 billion, and the company is still in the middle of an intense cost-cutting drive.

    Intel CFO David Zinsner and product chief MJ Holthaus currently serve as interim co-CEOs.
    "#;

    let query = "Eric";

    let query = "A";
    let query = "BRK-B";

    let results = extract_tickers_from_text(&query, &company_symbols_list).unwrap();

    println!("Extracted Tickers:");
    for (ticker_symbol, frequency) in results {
        println!("{}: {:.2}", ticker_symbol, frequency);
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/config.rs
-------------------------------------
use crate::models::CompanyTokenProcessorConfig;

pub const DEFAULT_COMPANY_TOKEN_PROCESSOR_CONFIG: &CompanyTokenProcessorConfig =
    &CompanyTokenProcessorConfig {
        threshold_ratio_exact_matches: 0.50,
        threshold_min_company_token_coverage: 0.60,
    };

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/constants.rs
-------------------------------------
pub const STOP_WORDS: &[&str] = &[
    "a",
    "about",
    "above",
    "after",
    "again",
    "against",
    "all",
    "am",
    "an",
    "and",
    "any",
    "are",
    "aren't",
    "as",
    "at",
    "be",
    "because",
    "been",
    "before",
    "being",
    "below",
    "between",
    "both",
    "but",
    "by",
    "can",
    "can't",
    "cannot",
    "could",
    "couldn't",
    "did",
    "didn't",
    "do",
    "does",
    "doesn't",
    "doing",
    "don't",
    "down",
    "during",
    "each",
    "few",
    "for",
    "from",
    "further",
    "had",
    "hadn't",
    "has",
    "hasn't",
    "have",
    "haven't",
    "having",
    "he",
    "he'd",
    "he'll",
    "he's",
    "her",
    "here",
    "here's",
    "hers",
    "herself",
    "him",
    "himself",
    "his",
    "how",
    "how's",
    "i",
    "i'd",
    "i'll",
    "i'm",
    "i've",
    "if",
    "in",
    "into",
    "is",
    "isn't",
    "it",
    "it's",
    "its",
    "itself",
    "let's",
    "me",
    "more",
    "most",
    "mustn't",
    "my",
    "myself",
    "no",
    "nor",
    "not",
    "of",
    "off",
    "on",
    "once",
    "only",
    "or",
    "other",
    "ought",
    "our",
    "ours",
    "ourselves",
    "out",
    "over",
    "own",
    "same",
    "shan't",
    "she",
    "she'd",
    "she'll",
    "she's",
    "should",
    "shouldn't",
    "so",
    "some",
    "such",
    "than",
    "that",
    "that's",
    "the",
    "their",
    "theirs",
    "them",
    "themselves",
    "then",
    "there",
    "there's",
    "these",
    "they",
    "they'd",
    "they'll",
    "they're",
    "they've",
    "this",
    "those",
    "through",
    "to",
    "too",
    "under",
    "until",
    "up",
    "very",
    "was",
    "wasn't",
    "we",
    "we'd",
    "we'll",
    "we're",
    "we've",
    "were",
    "weren't",
    "what",
    "what's",
    "when",
    "when's",
    "where",
    "where's",
    "which",
    "while",
    "who",
    "who's",
    "whom",
    "why",
    "why's",
    "with",
    "won't",
    "would",
    "wouldn't",
    "you",
    "you'd",
    "you'll",
    "you're",
    "you've",
    "your",
    "yours",
    "yourself",
    "yourselves",
];

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/lib.rs
-------------------------------------
mod config;
pub use config::DEFAULT_COMPANY_TOKEN_PROCESSOR_CONFIG;
mod constants;
pub mod models;
pub use models::{
    CompanyTokenMapper, CompanyTokenProcessor, CompanyTokenProcessorConfig, Error, TokenMapper,
    TokenParityState, TokenRangeState, Tokenizer,
};
pub mod types;
mod utils;
pub use types::{
    AlternateCompanyName, CompanyName, CompanySymbolList, TickerSymbol, TickerSymbolFrequencyMap,
    Token, TokenId, TokenRef, TokenVector,
};

pub fn extract_tickers_from_text(
    text: &str,
    company_symbols_list: &CompanySymbolList,
) -> Result<TickerSymbolFrequencyMap, Error> {
    let results_ticker_symbol_frequency_map = extract_tickers_from_text_with_custom_config(
        DEFAULT_COMPANY_TOKEN_PROCESSOR_CONFIG,
        &text,
        &company_symbols_list,
    )?;

    Ok(results_ticker_symbol_frequency_map)
}

pub fn extract_tickers_from_text_with_custom_config(
    document_token_processor_config: &CompanyTokenProcessorConfig,
    text: &str,
    company_symbols_list: &CompanySymbolList,
) -> Result<TickerSymbolFrequencyMap, Error> {
    let mut company_token_processor =
        CompanyTokenProcessor::new(document_token_processor_config, company_symbols_list);

    let results_ticker_symbol_frequency_map = company_token_processor.process_text_doc(text)?;

    Ok(results_ticker_symbol_frequency_map)
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/company_token_mapper.rs
-------------------------------------
use std::collections::HashMap;

use crate::types::{CompanySequenceIndex, CompanySymbolList, TickerSymbol, TokenId};

use crate::{TokenMapper, Tokenizer};

// TODO: Precompute during compile time, not run time
pub struct CompanyTokenMapper {
    pub token_mapper: TokenMapper,
    pub ticker_symbol_tokenizer: Tokenizer,
    pub text_doc_tokenizer: Tokenizer,
    pub ticker_symbol_map: HashMap<TickerSymbol, TokenId>,
    pub reverse_ticker_symbol_map: HashMap<TokenId, TickerSymbol>,
    // TODO: Replace tickersymbol with a token ID representing the ticker
    // symbol, and use the reverse ticker symbol map to map them back?
    pub company_token_sequences_map: HashMap<TickerSymbol, Vec<Vec<TokenId>>>,
    pub company_reverse_token_map: HashMap<TokenId, Vec<TickerSymbol>>,
}

impl CompanyTokenMapper {
    pub fn new(company_symbol_list: &CompanySymbolList) -> Self {
        let token_mapper = TokenMapper::new();

        let ticker_symbol_tokenizer = Tokenizer::ticker_symbol_parser();
        let text_doc_tokenizer = Tokenizer::text_doc_parser();

        let mut instance = CompanyTokenMapper {
            token_mapper,
            ticker_symbol_tokenizer,
            text_doc_tokenizer,
            ticker_symbol_map: HashMap::with_capacity(company_symbol_list.len()),
            reverse_ticker_symbol_map: HashMap::with_capacity(company_symbol_list.len()),
            company_token_sequences_map: HashMap::with_capacity(company_symbol_list.len()),
            company_reverse_token_map: HashMap::new(),
        };

        instance.ingest_company_tokens(&company_symbol_list);

        instance
    }

    fn clear(&mut self) {
        self.company_token_sequences_map.clear();
        self.company_reverse_token_map.clear();
        self.ticker_symbol_map.clear();
        self.reverse_ticker_symbol_map.clear();
    }

    /// Ingests tokens from the company symbol list
    fn ingest_company_tokens(&mut self, company_symbol_list: &CompanySymbolList) {
        self.clear();

        for (ticker_symbol, company_name, alt_company_names) in company_symbol_list {
            // let company_name_key = company_name.clone().unwrap();

            let mut all_company_name_token_ids = Vec::new();

            // Tokenize the ticker symbol and upsert token IDs
            let ticker_symbol_tokens = self.ticker_symbol_tokenizer.tokenize(&ticker_symbol);
            for ticker_symbol_token in ticker_symbol_tokens {
                let ticker_symbol_token_id = self.token_mapper.upsert_token(&ticker_symbol_token);

                self.ticker_symbol_map
                    .insert(ticker_symbol.clone(), ticker_symbol_token_id);

                self.reverse_ticker_symbol_map
                    .insert(ticker_symbol_token_id, ticker_symbol.clone());
            }

            if let Some(company_name) = company_name {
                let company_name_token_ids = self.process_company_name_tokens(&company_name);
                all_company_name_token_ids.push(company_name_token_ids.clone());

                // Populate reverse map
                for token_id in company_name_token_ids {
                    self.company_reverse_token_map
                        .entry(token_id.clone())
                        .or_insert_with(Vec::new)
                        .push(ticker_symbol.clone());
                }
            }

            // Process alternate company names
            for alt_company_name in alt_company_names {
                let alt_company_name_token_ids =
                    self.process_company_name_tokens(&alt_company_name);
                all_company_name_token_ids.push(alt_company_name_token_ids.clone());

                // Populate reverse map
                for token_id in alt_company_name_token_ids {
                    self.company_reverse_token_map
                        .entry(token_id)
                        .or_insert_with(Vec::new)
                        .push(ticker_symbol.clone());
                }
            }

            // Insert the collected token IDs into the map
            self.company_token_sequences_map
                .entry(ticker_symbol.clone())
                .or_insert_with(Vec::new)
                .extend(all_company_name_token_ids);
        }
    }

    /// Helper method for per-company token ingestion
    fn process_company_name_tokens(&mut self, company_name: &str) -> Vec<TokenId> {
        let company_name_tokens = self.text_doc_tokenizer.tokenize(&company_name);
        let mut company_name_token_ids = Vec::new();
        for token in company_name_tokens {
            let token_id = self.token_mapper.upsert_token(&token);
            company_name_token_ids.push(token_id);
        }

        company_name_token_ids
    }

    // TODO: Use actual error type (or return option type)
    pub fn get_ticker_symbol_by_token_id(
        &self,
        token_id: &TokenId,
    ) -> Result<&TickerSymbol, String> {
        match self.reverse_ticker_symbol_map.get(token_id) {
            Some(ticker_symbol) => Ok(ticker_symbol),
            None => Err("Could not obtain token id".to_string()),
        }
    }

    // TODO: Use actual error type (or return option type)
    pub fn get_ticker_symbol_token_id(
        &self,
        ticker_symbol: &TickerSymbol,
    ) -> Result<&TokenId, String> {
        match self.ticker_symbol_map.get(ticker_symbol) {
            Some(token_id) => Ok(token_id),
            None => Err("Could not obtain token id".to_string()),
        }
    }

    pub fn get_company_token_sequence_max_length(
        &self,
        ticker_symbol: &TickerSymbol,
        company_sequence_idx: CompanySequenceIndex,
    ) -> Option<usize> {
        self.company_token_sequences_map
            .get(ticker_symbol)
            .and_then(|seq| seq.get(company_sequence_idx).map(|s| s.len()))
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/company_token_processor.rs
-------------------------------------
use crate::types::{
    CompanySequenceIndex, CompanySymbolList, TickerSymbol, TickerSymbolFrequencyMap, Token, TokenId,
};
use crate::utils::{count_ticker_symbol_frequencies, dedup_vector};
use crate::{CompanyTokenMapper, Error, TokenParityState, TokenRangeState};

use log::info;
use std::collections::HashMap;

pub struct CompanyTokenProcessorConfig {
    // TODO: Rename
    pub threshold_ratio_exact_matches: f32,
    pub threshold_min_company_token_coverage: f32,
}

pub struct CompanyTokenProcessor<'a> {
    config: &'a CompanyTokenProcessorConfig,
    company_token_mapper: CompanyTokenMapper,
}

impl<'a> CompanyTokenProcessor<'a> {
    pub fn new(
        config: &'a CompanyTokenProcessorConfig,
        company_symbol_list: &'a CompanySymbolList,
    ) -> Self {
        let company_token_mapper = CompanyTokenMapper::new(&company_symbol_list);

        CompanyTokenProcessor {
            config,
            company_token_mapper,
        }
    }

    pub fn process_text_doc(&mut self, text: &str) -> Result<TickerSymbolFrequencyMap, Error> {
        // Tokenize the input text
        info!("Tokenizing...");
        let ticker_symbol_tokens = self
            .company_token_mapper
            .ticker_symbol_tokenizer
            .tokenize(text);
        let text_doc_tokens = self.company_token_mapper.text_doc_tokenizer.tokenize(text);

        info!("Gathering filtered tokens...");
        let (query_text_doc_token_ids, mut query_ticker_symbol_token_ids) =
            self.get_filtered_query_token_ids(&ticker_symbol_tokens, &text_doc_tokens)?;

        // Identify token ID sequences which start with the first token of a company token sequence
        info!("Identifying token ID sequences...");
        let potential_token_id_sequences =
            self.get_potential_token_sequences(&query_text_doc_token_ids);

        // Aggregate token parity states
        info!("Collecting token parity states...");
        let token_parity_states = TokenParityState::collect_token_parity_states(
            &query_text_doc_token_ids,
            &potential_token_id_sequences,
        );

        // Determine range states
        info!("Collecting token range states...");
        let mut token_range_states = TokenRangeState::collect_token_range_states(
            &self.company_token_mapper,
            &potential_token_id_sequences,
            &token_parity_states,
        );

        // Assign scores to the range states
        info!("Assigning range scores...");
        TokenRangeState::assign_token_range_scores(
            &query_text_doc_token_ids,
            &mut token_range_states,
        );

        // println!("TOKEN RANGE STATES");
        // for token_range_state in TokenRangeState::get_unique(&token_range_states) {
        //     println!("{:?}", token_range_state);
        // }
        // println!("----");

        // Discard token range states which do not meet minimum threshold
        token_range_states.retain(|state| {
            state.company_token_coverage >= self.config.threshold_min_company_token_coverage
        });

        // Collect top range states
        info!("Collecting top range states...");
        let top_range_states = TokenRangeState::collect_top_range_states(
            &query_text_doc_token_ids,
            &token_range_states,
        );

        // TODO: Remove
        // println!("Top Range States for Each Query Token Index:");
        for state in &top_range_states {
            println!("{:?}", state);
        }

        // Used to determine whether to explicitly parse out symbols which may also be stop words, based on
        // percentage of symbols to company names in the doc (for instance, determine if "A" should be parsed
        // as a symbol)
        let ratio_exact_matches =
            TokenRangeState::calc_exact_ticker_symbol_match_ratio(&top_range_states);

        // Clear exact ticker symbol matches if ratio of exact matches is less than configured minimum
        if ratio_exact_matches < self.config.threshold_ratio_exact_matches {
            query_ticker_symbol_token_ids = vec![];
        }

        // Keep track of number of occurrences, per extracted symbol, for context stats
        let text_doc_ticker_frequencies =
            TokenRangeState::count_token_range_ticker_symbol_frequencies(&top_range_states);

        // TODO: Don't use unwrap
        let query_ticker_symbols: Vec<&String> = query_ticker_symbol_token_ids
            .iter()
            .map(|token_id| {
                self.company_token_mapper
                    .get_ticker_symbol_by_token_id(token_id)
                    .unwrap()
            })
            .collect();

        let unique_query_ticker_symbols = dedup_vector(&query_ticker_symbols);

        let unique_text_doc_ticker_symbols: Vec<TickerSymbol> =
            text_doc_ticker_frequencies.keys().cloned().collect();

        // TODO: Remove
        println!(
            "ticker_symbol_tokens: {:?}, query_text_doc_token_ids: {:?}, query_text_doc_tokens: {:?}, query_ticker_symbols: {:?}, unique_query_ticker_symbols: {:?}, text_doc_ticker_frequencies: {:?}, ratio_exact_matches: {}, match_threshold: {}",
            ticker_symbol_tokens, query_text_doc_token_ids, self.company_token_mapper.token_mapper.get_tokens_by_ids(&query_text_doc_token_ids), &query_ticker_symbols, &unique_query_ticker_symbols, text_doc_ticker_frequencies, ratio_exact_matches, self.config.threshold_ratio_exact_matches
        );

        let query_tickers_not_in_text_doc: Vec<&TickerSymbol> = unique_query_ticker_symbols
            .clone()
            .into_iter()
            .filter(|symbol| !unique_text_doc_ticker_symbols.contains(symbol))
            .collect();

        let query_tickers_not_in_text_doc: Vec<TickerSymbol> = query_tickers_not_in_text_doc
            .iter()
            .cloned()
            .cloned()
            .collect();
        let mut query_ticker_frequencies =
            count_ticker_symbol_frequencies(&query_tickers_not_in_text_doc);

        self.adjust_query_ticker_frequencies(&mut query_ticker_frequencies, &top_range_states)?;

        let combined_ticker_frequencies = self.combine_ticker_symbol_frequencies(&[
            text_doc_ticker_frequencies.clone(),
            query_ticker_frequencies.clone(),
        ]);

        // TODO: Remove
        println!(
            "unique_text_doc_ticker_symbols: {:?}, unique_query_ticker_symbols: {:?}, query_tickers_not_in_text_doc: {:?}, text_doc_ticker_frequencies: {:?}, query_ticker_frequencies: {:?}, combined_ticker_frequencies: {:?}",
            unique_text_doc_ticker_symbols, unique_query_ticker_symbols, query_tickers_not_in_text_doc, text_doc_ticker_frequencies, query_ticker_frequencies, combined_ticker_frequencies
        );

        Ok(combined_ticker_frequencies)
    }

    /// Reduces query ticker frequency counts based on matches in top range states
    fn adjust_query_ticker_frequencies(
        &self,
        query_ticker_frequencies: &mut TickerSymbolFrequencyMap,
        top_range_states: &[TokenRangeState],
    ) -> Result<(), Error> {
        for range_state in top_range_states {
            let range_text_doc_token_ids = &range_state.query_text_doc_token_ids;

            for (query_ticker_symbol, query_ticker_symbol_frequency) in
                query_ticker_frequencies.iter_mut()
            {
                // TODO: Don't use unwrap
                let query_ticker_symbol_token_id = self
                    .company_token_mapper
                    .get_ticker_symbol_token_id(query_ticker_symbol)
                    .unwrap();

                if range_text_doc_token_ids.contains(&query_ticker_symbol_token_id) {
                    *query_ticker_symbol_frequency =
                        query_ticker_symbol_frequency.saturating_sub(1);
                }
            }
        }

        // Remove entries with zero frequencies
        query_ticker_frequencies.retain(|_, &mut frequency| frequency > 0);

        Ok(())
    }

    fn combine_ticker_symbol_frequencies(
        &self,
        ticker_symbol_frequency_hash_maps: &[TickerSymbolFrequencyMap],
    ) -> TickerSymbolFrequencyMap {
        let mut combined_ticker_frequencies: HashMap<TickerSymbol, usize> = HashMap::new();

        for frequency_hash_map in ticker_symbol_frequency_hash_maps {
            for (ticker_symbol, frequency) in frequency_hash_map {
                *combined_ticker_frequencies
                    .entry(ticker_symbol.clone())
                    .or_insert(0) += frequency;
            }
        }

        combined_ticker_frequencies
    }

    // TODO: Keep? If so, use logger method
    // #[allow(dead_code)]
    // /// For debugging purposes
    // fn display_company_tokens(&self, ticker_symbol: &TickerSymbol) {
    //     if let Some(company_token_sequences) = self.company_token_sequences.get(ticker_symbol) {
    //         for company_token_sequence in company_token_sequences {
    //             println!(
    //                 "{:?}",
    //                 self.token_mapper.get_tokens_by_ids(company_token_sequence)
    //             );
    //         }
    //     } else {
    //         println!("No tokens found for ticker symbol: {}", ticker_symbol);
    //     }
    // }

    // TODO: Don't return Result type; no error will be thrown
    fn get_filtered_query_token_ids(
        &self,
        ticker_symbol_tokens: &Vec<Token>,
        text_doc_tokens: &Vec<Token>,
    ) -> Result<(Vec<TokenId>, Vec<TokenId>), Error> {
        // Get the filtered token IDs (IDs present in the TokenMapper)
        let query_text_doc_token_ids = self
            .company_token_mapper
            .token_mapper
            .get_filtered_token_ids(text_doc_tokens.iter().map(|s| s.as_str()).collect());

        let query_ticker_symbol_token_ids = self
            .company_token_mapper
            .token_mapper
            .get_filtered_token_ids(ticker_symbol_tokens.iter().map(|s| s.as_str()).collect());

        Ok((query_text_doc_token_ids, query_ticker_symbol_token_ids))
    }

    fn get_potential_token_sequences(
        &self,
        query_text_doc_token_ids: &[TokenId],
    ) -> HashMap<TickerSymbol, Vec<(CompanySequenceIndex, Vec<TokenId>)>> {
        let mut potential_token_id_sequences: HashMap<
            TickerSymbol,
            Vec<(CompanySequenceIndex, Vec<TokenId>)>,
        > = HashMap::new();

        for query_token_id in query_text_doc_token_ids {
            if let Some(possible_ticker_symbols) = self
                .company_token_mapper
                .company_reverse_token_map
                .get(query_token_id)
            {
                for ticker_symbol in possible_ticker_symbols {
                    if let Some(company_name_variations_token_ids_list) = self
                        .company_token_mapper
                        .company_token_sequences_map
                        .get(ticker_symbol)
                    {
                        for (company_sequence_idx, company_name_variations_token_ids) in
                            company_name_variations_token_ids_list.iter().enumerate()
                        {
                            if company_name_variations_token_ids.is_empty() {
                                continue;
                            }

                            let company_name_first_token_id = company_name_variations_token_ids[0];

                            if *query_token_id == company_name_first_token_id {
                                // Add or update the hashmap entry for this ticker_symbol
                                potential_token_id_sequences
                                    .entry(ticker_symbol.clone())
                                    .or_insert_with(Vec::new) // Create an empty Vec if the key doesn't exist
                                    .retain(|(existing_idx, existing_vec)| {
                                        *existing_idx != company_sequence_idx
                                            || *existing_vec != *company_name_variations_token_ids
                                    }); // Remove duplicates

                                // TODO: Don't use unwrap
                                if !potential_token_id_sequences
                                    .get(&ticker_symbol.clone())
                                    .unwrap()
                                    .iter()
                                    .any(|(existing_idx, existing_vec)| {
                                        *existing_idx == company_sequence_idx
                                            && *existing_vec == *company_name_variations_token_ids
                                    })
                                {
                                    potential_token_id_sequences
                                        .get_mut(&ticker_symbol.to_string())
                                        .unwrap()
                                        .push((
                                            company_sequence_idx,
                                            company_name_variations_token_ids.clone(),
                                        ));
                                }
                            }
                        }
                    }
                }
            }
        }

        potential_token_id_sequences
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/error.rs
-------------------------------------
// TODO: Consider using structs instead of enum for specific Result error types

use std::fmt;

#[derive(Debug)]
pub enum Error {
    ParserError(String),
    TokenFilterError(String),
    // CoverageAnalysisError(String),
    // ConfidenceAnalysisError(String),
    // MissingDataError(String),
    // IoError(std::io::Error),
    // Other(String),
}

impl fmt::Display for Error {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Error::ParserError(msg) => write!(f, "Parser Error: {}", msg),
            Error::TokenFilterError(msg) => write!(f, "Token Filter Error: {}", msg),
            // Error::CoverageAnalysisError(msg) => write!(f, "Coverage Analysis Error: {}", msg),
            // Error::ConfidenceAnalysisError(msg) => {
            //     write!(f, "Confidence Analysis Error: {}", msg)
            // }
            // Error::MissingDataError(msg) => write!(f, "Missing Data Error: {}", msg),
            // Error::IoError(err) => write!(f, "IO Error: {}", err),
            // Error::Other(msg) => write!(f, "Other Error: {}", msg),
        }
    }
}

impl From<String> for Error {
    fn from(msg: String) -> Error {
        Error::ParserError(msg)
    }
}

impl From<&str> for Error {
    fn from(msg: &str) -> Error {
        Error::ParserError(msg.to_string())
    }
}

// impl From<std::io::Error> for Error {
//     fn from(err: std::io::Error) -> Error {
//         Error::IoError(err)
//     }
// }

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models.rs
-------------------------------------
pub mod error;
pub use error::Error;

pub mod company_token_processor;
pub use company_token_processor::{CompanyTokenProcessor, CompanyTokenProcessorConfig};

pub mod tokenizer;
pub use tokenizer::Tokenizer;

pub mod token_mapper;
pub use token_mapper::TokenMapper;

pub mod company_token_mapper;
pub use company_token_mapper::CompanyTokenMapper;

pub mod token_parity_state;
pub use token_parity_state::TokenParityState;

pub mod token_range_state;
pub use token_range_state::TokenRangeState;

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/tokenizer.rs
-------------------------------------
use crate::constants::STOP_WORDS;
use crate::types::{Token, TokenRef, TokenVector};
use std::char;
use std::collections::HashSet;

pub struct Tokenizer {
    min_uppercase_ratio: Option<f32>,
    pre_processed_stop_words: Option<HashSet<String>>,
}

impl Tokenizer {
    /// Configuration specifically for ticker symbol parsing
    pub fn ticker_symbol_parser() -> Self {
        Self {
            min_uppercase_ratio: Some(0.9),
            pre_processed_stop_words: None,
        }
    }
    /// Configuration for arbitrary text doc parsing
    pub fn text_doc_parser() -> Self {
        Self {
            min_uppercase_ratio: None,
            pre_processed_stop_words: Some(Self::preprocess_stop_words()),
        }
    }

    /// Tokenizer function to split the text into individual tokens.
    pub fn tokenize(&self, text: &str) -> Vec<Token> {
        let stop_words = self.pre_processed_stop_words.as_ref();

        // Preprocess and tokenize the text
        text.replace("-\n", "") // Merge hyphenated words across lines
            .replace('\n', " ") // Normalize line breaks to spaces
            .replace('\r', " ") // Handle potential carriage returns
            .replace("--", " ") // Replace standalone double hyphens
            .replace(",", " ") // Normalize commas to spaces
            .split_whitespace() // Split into words
            // Remove possessive endings
            .map(|word| {
                let stripped = word.replace("'s", "").replace("s'", "");

                stripped
                    .chars()
                    .filter(|c| c.is_alphanumeric())
                    .collect::<String>()
            })
            .filter(|word| {
                // Apply uppercase ratio filter and any capital letter requirement
                let passes_uppercase_ratio = self
                    .min_uppercase_ratio
                    .map_or(true, |ratio| self.calc_uppercase_ratio(word) >= ratio);

                let passes_any_caps_or_is_number =
                    word.chars().any(|c| c.is_uppercase()) || word.chars().all(|c| c.is_numeric());

                passes_uppercase_ratio && passes_any_caps_or_is_number
            })
            // Split hyphenated words into multiple words
            .flat_map(|word| {
                let parts: Vec<String> = word
                    .split('-')
                    .map(|part| {
                        part.chars()
                            .filter(|c| c.is_alphanumeric())
                            .collect::<String>()
                    })
                    .collect();

                if parts.len() > 1 {
                    parts.into_iter() // Only split into parts if there are multiple segments
                } else {
                    vec![word.replace('-', "")].into_iter() // Otherwise, use the whole word
                }
            })
            // Filter to alphanumeric and uppercase
            .map(|word| {
                word.chars()
                    .filter(|c| c.is_alphanumeric())
                    .collect::<String>() // Collect filtered characters into a String
                    .to_uppercase() // Convert to uppercase
            })
            // Skip empty words and stop words
            .filter(|word| !word.is_empty() && stop_words.map_or(true, |sw| !sw.contains(word)))
            .collect()
    }

    fn calc_uppercase_ratio(&self, word: &TokenRef) -> f32 {
        let total_chars = word.chars().count() as f32;
        if total_chars == 0.0 {
            return 0.0;
        }
        let uppercase_chars = word.chars().filter(|c| c.is_uppercase()).count() as f32;
        uppercase_chars / total_chars
    }

    /// Pre-process the stop words by removing non-alphanumeric characters and converting to uppercase
    fn preprocess_stop_words() -> HashSet<String> {
        STOP_WORDS
            .iter()
            .map(|word| {
                word.chars()
                    .filter(|c| c.is_alphanumeric())
                    .collect::<String>()
                    .to_uppercase()
            })
            .collect()
    }

    pub fn tokenize_to_charcode_vectors(&self, text: &TokenRef) -> Vec<TokenVector> {
        self.tokenize(text)
            .iter() // Use the existing `tokenize` function to get tokens
            .map(|token| Tokenizer::token_to_charcode_vector(&token))
            .collect()
    }

    pub fn token_to_charcode_vector(token: &TokenRef) -> TokenVector {
        token.chars().map(|c| c as u32).collect()
    }

    pub fn tokens_to_charcode_vectors(tokens: &Vec<&TokenRef>) -> Vec<TokenVector> {
        tokens
            .iter()
            .map(|token| Tokenizer::token_to_charcode_vector(token))
            .collect()
    }

    pub fn charcode_vector_to_token(charcodes: &TokenVector) -> Token {
        charcodes
            .iter()
            .map(|&code| char::from_u32(code).unwrap_or('\u{FFFD}')) // Convert code to char, using '�' as a fallback
            .collect()
    }

    pub fn charcode_vectors_to_tokens(charcode_vectors: &Vec<TokenVector>) -> Vec<Token> {
        charcode_vectors
            .iter()
            .map(|charcodes| Tokenizer::charcode_vector_to_token(charcodes))
            .collect() // Collect the resulting strings into a Vec<String>
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/token_mapper.rs
-------------------------------------
use crate::types::{TokenId, TokenRef, TokenVector};
use crate::Tokenizer;
use std::collections::HashMap;

/// Maps "global" tokens without including function specific to token types.
pub struct TokenMapper {
    token_map: HashMap<TokenVector, TokenId>,
    reverse_token_map: HashMap<TokenId, TokenVector>,
    next_id: TokenId,
}

impl TokenMapper {
    /// Creates a new TokenMapper
    pub fn new() -> Self {
        TokenMapper {
            token_map: HashMap::new(),
            reverse_token_map: HashMap::new(),
            next_id: 0,
        }
    }

    /// Adds a token (as a string) to the map if it doesn't exist,
    /// and returns its unique ID
    pub fn upsert_token(&mut self, token: &str) -> TokenId {
        let token_vector = Tokenizer::token_to_charcode_vector(token);

        if let Some(&id) = self.token_map.get(&token_vector) {
            id
        } else {
            let id = self.next_id;
            self.token_map.insert(token_vector.clone(), id);
            self.reverse_token_map.insert(id, token_vector.clone());
            self.next_id += 1;
            id
        }
    }

    /// Gets the ID for a token (as a string), or None if the token is not present
    pub fn get_token_id(&self, token: &TokenRef) -> Option<TokenId> {
        let token_vector = Tokenizer::token_to_charcode_vector(token);

        self.token_map.get(&token_vector).copied()
    }

    pub fn get_filtered_tokens<'a>(&'a self, tokens: Vec<&'a TokenRef>) -> Vec<&TokenRef> {
        tokens
            .into_iter()
            .filter(|token| self.get_token_id(token).is_some())
            .collect()
    }

    pub fn get_filtered_token_ids<'a>(&'a self, tokens: Vec<&'a TokenRef>) -> Vec<TokenId> {
        tokens
            .into_iter()
            .filter_map(|token| self.get_token_id(token))
            .collect()
    }

    pub fn get_token_by_id(&self, token_id: TokenId) -> Option<String> {
        self.reverse_token_map
            .get(&token_id)
            .map(|token_vector| Tokenizer::charcode_vector_to_token(token_vector))
    }

    pub fn get_tokens_by_ids(&self, token_ids: &[TokenId]) -> Vec<Option<String>> {
        token_ids
            .iter()
            .map(|&token_id| self.get_token_by_id(token_id))
            .collect()
    }

    /// Gets the total number of unique tokens
    pub fn get_token_count(&self) -> usize {
        self.token_map.len()
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/token_parity_state.rs
-------------------------------------
use crate::types::{
    CompanySequenceIndex, CompanySequenceTokenIndex, QueryTokenIndex, TickerSymbol, TokenId,
};
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct TokenParityState {
    pub ticker_symbol: TickerSymbol,
    pub query_token_idx: QueryTokenIndex,
    pub query_token_id: TokenId,
    pub company_sequence_idx: CompanySequenceIndex,
    pub company_sequence_token_idx: CompanySequenceTokenIndex,
}

impl TokenParityState {
    pub fn collect_token_parity_states(
        query_text_doc_token_ids: &[TokenId],
        potential_token_id_sequences: &HashMap<
            TickerSymbol,
            Vec<(CompanySequenceIndex, Vec<TokenId>)>,
        >,
    ) -> Vec<TokenParityState> {
        let mut token_parity_states = Vec::new();

        for (ticker_symbol, company_token_sequences) in potential_token_id_sequences {
            for company_sequence_tuple in company_token_sequences {
                for (query_token_idx, query_token_id) in query_text_doc_token_ids.iter().enumerate()
                {
                    let company_sequence_idx = &company_sequence_tuple.0;
                    let company_sequence_token_ids = &company_sequence_tuple.1;

                    for (company_sequence_token_idx, company_sequence_token_id) in
                        company_sequence_token_ids.iter().enumerate()
                    {
                        if company_sequence_token_id == query_token_id {
                            token_parity_states.push(TokenParityState {
                                ticker_symbol: ticker_symbol.to_string(),
                                query_token_idx,
                                query_token_id: *query_token_id,
                                company_sequence_idx: *company_sequence_idx,
                                company_sequence_token_idx,
                            });
                        }
                    }
                }
            }
        }

        // Reorder token_parity_states
        token_parity_states.sort_by(|a, b| {
            (
                &a.ticker_symbol,
                a.company_sequence_idx,
                a.query_token_idx,
                a.company_sequence_token_idx,
            )
                .cmp(&(
                    &b.ticker_symbol,
                    b.company_sequence_idx,
                    b.query_token_idx,
                    b.company_sequence_token_idx,
                ))
        });

        token_parity_states
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/models/token_range_state.rs
-------------------------------------
use std::collections::{HashMap, HashSet};

use crate::types::{
    CompanySequenceIndex, CompanySequenceTokenIndex, QueryTokenIndex, TickerSymbol,
    TickerSymbolFrequencyMap, Token, TokenId,
};
use crate::utils::count_ticker_symbol_frequencies;
use crate::{CompanyTokenMapper, TokenParityState};

#[derive(Debug, Clone)]
pub struct TokenRangeState {
    pub ticker_symbol: TickerSymbol,
    pub ticker_symbol_token_id: TokenId,
    pub is_matched_on_ticker_symbol: Option<bool>,
    // TODO: Track TD-IDF scores of query tokens in relation to the query itself?
    // TODO: Track vector_similarity_state_indices?
    // vector_similarity_states: Vec<QueryVectorIntermediateSimilarityState>,
    pub query_token_indices: Vec<QueryTokenIndex>,
    pub query_text_doc_token_ids: Vec<TokenId>,
    pub company_sequence_idx: CompanySequenceIndex,
    pub company_sequence_token_indices: Vec<CompanySequenceTokenIndex>,
    pub company_sequence_max_length: usize,
    pub company_token_coverage: f32,
    pub range_score: Option<f32>,
    // TODO: Consider renaming; I believe range scores are set after finalization
    pub is_finalized: bool,
}

impl TokenRangeState {
    pub fn new(
        ticker_symbol: TickerSymbol,
        ticker_symbol_token_id: TokenId,
        company_sequence_idx: CompanySequenceIndex,
        company_sequence_max_length: usize,
    ) -> Self {
        TokenRangeState {
            ticker_symbol,
            ticker_symbol_token_id,
            is_matched_on_ticker_symbol: None,
            query_token_indices: vec![],
            query_text_doc_token_ids: vec![],
            company_sequence_idx,
            company_sequence_token_indices: vec![],
            company_sequence_max_length,
            company_token_coverage: 0.0,
            range_score: None,
            is_finalized: false,
        }
    }

    pub fn add_partial_state(
        &mut self,
        query_token_idx: QueryTokenIndex,
        query_token_id: TokenId,
        company_sequence_token_idx: CompanySequenceTokenIndex,
    ) {
        self.query_token_indices.push(query_token_idx);
        self.query_text_doc_token_ids.push(query_token_id);
        self.company_sequence_token_indices
            .push(company_sequence_token_idx);
    }

    pub fn calc_exact_ticker_symbol_match_ratio(top_token_range_states: &[TokenRangeState]) -> f32 {
        if top_token_range_states.is_empty() {
            return 1.0;
        }

        let (exact_matches, total) =
            top_token_range_states
                .iter()
                .fold((0, 0), |(exact_matches, total), state| {
                    (
                        exact_matches
                            + if state.is_matched_on_ticker_symbol == Some(true) {
                                1
                            } else {
                                0
                            },
                        total + 1,
                    )
                });

        let ratio_exact_matches = if total > 0 {
            exact_matches as f32 / total as f32
        } else {
            0.0
        };

        ratio_exact_matches
    }

    /// Given a vector of token range states, this counts the number of symbols iwth unique query token indices
    pub fn count_token_range_ticker_symbol_frequencies(
        range_states: &[TokenRangeState],
    ) -> TickerSymbolFrequencyMap {
        // Step 1: Deduplicate query token indices for each ticker symbol
        let mut ticker_symbol_query_indices: HashMap<TickerSymbol, HashSet<Vec<QueryTokenIndex>>> =
            HashMap::new();

        for state in range_states {
            let ticker_symbol = state.ticker_symbol.clone();
            let query_token_indices = state.query_token_indices.clone();

            ticker_symbol_query_indices
                .entry(ticker_symbol)
                .or_insert_with(HashSet::new)
                .insert(query_token_indices);
        }

        // Step 2: Flatten deduplicated indices into a list of ticker symbols
        let ticker_symbols: Vec<TickerSymbol> = ticker_symbol_query_indices
            .into_iter()
            .flat_map(|(ticker_symbol, query_index_sets)| {
                query_index_sets
                    .into_iter()
                    .map(move |_| ticker_symbol.clone())
            })
            .collect();

        // Step 3: Count frequencies using the wrapper
        count_ticker_symbol_frequencies(&ticker_symbols)
    }

    pub fn collect_top_range_states(
        query_text_doc_token_ids: &[TokenId],
        token_range_states: &[TokenRangeState],
    ) -> Vec<TokenRangeState> {
        let mut top_range_states_map: Vec<Vec<&TokenRangeState>> =
            vec![Vec::new(); query_text_doc_token_ids.len()];

        for token_range_state in token_range_states {
            for &query_token_idx in &token_range_state.query_token_indices {
                if let Some(range_score) = token_range_state.range_score {
                    if top_range_states_map[query_token_idx].is_empty() {
                        // Initialize with the current range state if no state exists
                        top_range_states_map[query_token_idx].push(token_range_state);
                    } else {
                        // TODO: Don't use unwrap
                        // Check the score of the existing states
                        let existing_score = top_range_states_map[query_token_idx][0]
                            .range_score
                            .unwrap();

                        if range_score > existing_score {
                            // Replace with a new top scorer
                            top_range_states_map[query_token_idx].clear();
                            top_range_states_map[query_token_idx].push(token_range_state);
                        } else if (range_score - existing_score).abs() < f32::EPSILON {
                            // Add to the top scorers in case of a tie
                            top_range_states_map[query_token_idx].push(token_range_state);
                        }
                    }
                }
            }
        }

        // Collect only the valid range states
        let top_range_states: Vec<&TokenRangeState> = top_range_states_map
            .into_iter()
            .flat_map(|states| states) // Flatten the vector of vectors
            .collect();

        top_range_states.into_iter().cloned().collect()
    }

    /// Determines the highest scores which map to each filtered token index.
    pub fn assign_token_range_scores(
        query_text_doc_token_ids: &[TokenId],
        token_range_states: &mut [TokenRangeState],
    ) {
        for (query_token_idx, _query_token_id) in query_text_doc_token_ids.iter().enumerate() {
            // Initialize a map to store scores for this token
            let mut token_scores: HashMap<Token, f32> = HashMap::new();

            // Iterate over all token range states
            for token_range_state in &mut *token_range_states {
                // Check if the current filtered token ID is part of the filtered token IDs in the range state
                if token_range_state
                    .query_token_indices
                    .contains(&query_token_idx)
                {
                    let score = token_range_state.company_token_coverage
                        // Increase score by continuity
                        // TODO: Weight this accordingly
                        + token_range_state
                            .query_token_indices
                            .iter()
                            .position(|&x| x == query_token_idx)
                            .map(|idx| idx as f32)
                            .unwrap_or(0.0);

                    // TODO: Remove
                    // println!("score: {:?}, state: {:?}", score, token_range_state);

                    token_range_state.range_score = Some(score);

                    // Update the score map for this ticker symbol
                    token_scores
                        .entry(token_range_state.ticker_symbol.clone())
                        .and_modify(|existing_score| {
                            *existing_score = (*existing_score).max(score);
                        })
                        .or_insert(score);
                }
            }

            // Filter token_scores to retain only the highest scores
            if !token_scores.is_empty() {
                let max_score = token_scores.values().cloned().fold(f32::MIN, f32::max); // Find the maximum score
                token_scores.retain(|_, &mut score| score == max_score); // Retain entries with the highest score
            }
        }
    }

    /// The returned vector represents unique token range states.
    pub fn collect_token_range_states(
        company_token_mapper: &CompanyTokenMapper,
        potential_token_id_sequences: &HashMap<
            TickerSymbol,
            Vec<(CompanySequenceIndex, Vec<TokenId>)>,
        >,
        token_parity_states: &[TokenParityState],
    ) -> Vec<TokenRangeState> {
        let mut token_range_states: Vec<TokenRangeState> = Vec::new();

        for (ticker_symbol, _) in potential_token_id_sequences {
            // TODO: Don't use unwrap here
            let ticker_symbol_token_id = company_token_mapper
                .get_ticker_symbol_token_id(&ticker_symbol)
                .unwrap();

            // Initialize state variables to track the last indices for continuity checks.
            let mut last_company_sequence_idx = usize::MAX - 1;
            let mut last_company_sequence_token_idx = usize::MAX - 1;
            let mut last_query_token_idx = usize::MAX - 1;

            // Indicates whether we are starting a new subsequence.
            // A subsequence is a group of contiguous tokens from the query and company sequences
            // that belong to the same ticker symbol and are aligned in both sequences.
            let mut is_new_sub_sequence = false;

            // Current token range state being constructed.
            let mut token_range_state: Option<TokenRangeState> = None;

            for token_parity_state in token_parity_states {
                if token_parity_state.ticker_symbol != *ticker_symbol {
                    last_company_sequence_idx = usize::MAX - 1;
                    last_company_sequence_token_idx = usize::MAX - 1;
                    last_query_token_idx = usize::MAX - 1;

                    continue;
                }

                is_new_sub_sequence = token_parity_state.company_sequence_token_idx == 0
                    || last_company_sequence_idx != token_parity_state.company_sequence_idx
                    || token_parity_state.company_sequence_token_idx
                        != last_company_sequence_token_idx + 1
                    || token_parity_state.query_token_idx != last_query_token_idx + 1;

                if is_new_sub_sequence {
                    // Finalize previous batch, if exists
                    if let Some(ref mut token_range_state) = token_range_state {
                        if !token_range_state.is_finalized {
                            token_range_state.finalize();

                            if !token_range_state.query_token_indices.is_empty() {
                                token_range_states.push(token_range_state.clone());
                            }
                        }
                    }

                    token_range_state = Some(TokenRangeState::new(
                        ticker_symbol.to_string(),
                        *ticker_symbol_token_id,
                        token_parity_state.company_sequence_idx,
                        company_token_mapper
                            .get_company_token_sequence_max_length(
                                ticker_symbol,
                                token_parity_state.company_sequence_idx,
                            )
                            // TODO: Replace with ?
                            .unwrap(),
                    ));
                }

                // Add partial state to the current token range state
                if let Some(ref mut token_range_state) = token_range_state {
                    // Only add the current token to the token range if:
                    // - It's not a new subsequence, or
                    // - It is the first token in the company sequence.
                    if !(is_new_sub_sequence && token_parity_state.company_sequence_token_idx != 0)
                    {
                        token_range_state.add_partial_state(
                            token_parity_state.query_token_idx,
                            token_parity_state.query_token_id,
                            token_parity_state.company_sequence_token_idx,
                        );
                    }
                }

                last_company_sequence_idx = token_parity_state.company_sequence_idx;
                last_company_sequence_token_idx = token_parity_state.company_sequence_token_idx;
                last_query_token_idx = token_parity_state.query_token_idx;

                is_new_sub_sequence = false;
            }

            // Finalize previous batch, if exists
            if let Some(ref mut token_range_state) = token_range_state {
                if !token_range_state.is_finalized {
                    token_range_state.finalize();

                    if !token_range_state.query_token_indices.is_empty() {
                        token_range_states.push(token_range_state.clone());
                    }
                }
            }
        }

        TokenRangeState::get_unique(&token_range_states)
    }

    // TODO: Make this non-public if possible
    pub fn finalize(&mut self) {
        self.update_coverage();

        self.is_matched_on_ticker_symbol = Some(
            self.query_text_doc_token_ids.len() == 1
                && self.query_text_doc_token_ids[0] == self.ticker_symbol_token_id,
        );

        self.is_finalized = true;
    }

    /// Recalculates the coverage based on the filtered indices and sequence length
    fn update_coverage(&mut self) {
        self.company_token_coverage =
            self.query_token_indices.len() as f32 / self.company_sequence_max_length as f32;
    }

    pub fn get_unique(token_range_states: &[TokenRangeState]) -> Vec<TokenRangeState> {
        // Use a HashSet to track unique combinations of ticker_symbol and query_text_doc_token_ids
        let mut seen = HashSet::new();
        let mut unique_states = Vec::new();

        for state in token_range_states {
            // Create a tuple representing the unique key
            let unique_key = (
                &state.ticker_symbol,
                &state.query_text_doc_token_ids,
                &state.company_sequence_idx,
            );

            // Check if this combination has been seen before
            if seen.insert(unique_key) {
                unique_states.push(state.clone());
            }
        }

        unique_states
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/types.rs
-------------------------------------
use std::collections::HashMap;

// Types listed here are either shared across multiple files and/or exposed via the library.

/// Represents a token as an owned `String`. Tokens are the basic units used for processing text.
pub type Token = String;

/// Represents a borrowed view of a token as a `str`. This is used when ownership is not required.
pub type TokenRef = str;

/// A vector of token IDs, represented as `u32`. This type is used to store sequences of token IDs
/// that map to specific tokens in a `TokenMapper`.
pub type TokenVector = Vec<u32>;

/// A unique identifier for a token, represented as a `usize`. This is typically used to index
/// tokens in a data structure such as a `HashMap` or `Vec`.
pub type TokenId = usize;

/// Represents the name of a company as an owned `String`.
pub type CompanyName = String;

/// Represents an alternate name for a company as an owned `String`. These are used to match
/// variations in naming conventions or aliases for companies.
pub type AlternateCompanyName = String;

/// A list of company symbols, where each entry includes:
/// - `TickerSymbol`: The company's stock ticker.
/// - `Option<CompanyName>`: The company's primary name (optional if not available).
/// - `Vec<AlternateCompanyName>`: A list of alternate names or aliases for the company.
pub type CompanySymbolList = Vec<(TickerSymbol, Option<CompanyName>, Vec<AlternateCompanyName>)>;

/// Represents a ticker symbol (e.g., stock ticker) as an owned `String`.
pub type TickerSymbol = String;

/// Represents the total number occurrences of a ticker symbol within a text document.
pub type TickerSymbolFrequency = usize;

/// Represents a map of ticker symbols to their frequency counts within a text document.
/// The key is the `TickerSymbol`, and the value is the `TickerSymbolFrequency`.
pub type TickerSymbolFrequencyMap = HashMap<TickerSymbol, TickerSymbolFrequency>;

// TODO: Document
pub type QueryTokenIndex = usize;
pub type CompanySequenceIndex = usize;
pub type CompanySequenceTokenIndex = usize;

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/utils/count_ticker_symbol_frequencies.rs
-------------------------------------
use crate::types::TickerSymbol;
use std::collections::HashMap;

pub fn count_ticker_symbol_frequencies(
    ticker_symbols: &[TickerSymbol],
) -> HashMap<TickerSymbol, usize> {
    let mut frequencies: HashMap<TickerSymbol, usize> = HashMap::new();

    for ticker_symbol in ticker_symbols {
        *frequencies.entry(ticker_symbol.clone()).or_insert(0) += 1;
    }

    frequencies
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/utils/dedup_vector.rs
-------------------------------------
use std::collections::HashSet;
use std::hash::Hash;

/// Deduplicates a vector while maintaining the original order.
///
/// # Arguments
/// * `vec` - A vector containing elements to deduplicate.
///
/// # Returns
/// A new vector with duplicates removed, preserving the original order.
pub fn dedup_vector<T: Eq + Hash + Clone>(vec: &[T]) -> Vec<T> {
    let mut seen = HashSet::new();
    vec.iter()
        .filter_map(|item| {
            if seen.insert(item) {
                Some(item.clone())
            } else {
                None
            }
        })
        .collect()
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/src/utils.rs
-------------------------------------
pub mod count_ticker_symbol_frequencies;
pub use count_ticker_symbol_frequencies::count_ticker_symbol_frequencies;

pub mod dedup_vector;
pub use dedup_vector::dedup_vector;

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/tests/test_ticker_extraction.rs
-------------------------------------
#[path = "../test_utils/lib.rs"]
mod test_utils;
use test_utils::constants::TEST_FILES_DIRECTORY;

use std::fs::read_dir;
use test_utils::run_test_for_file;
use ticker_sniffer::DEFAULT_COMPANY_TOKEN_PROCESSOR_CONFIG;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_extract_tickers_from_multiple_files() {
        println!("Testing ticker extractions...");

        // Directory containing the test files
        let test_dir = TEST_FILES_DIRECTORY;

        // Read all files in the directory
        let files = read_dir(test_dir)
            .expect("Failed to read test files directory")
            .collect::<Result<Vec<_>, _>>()
            .expect("Failed to collect directory entries");

        let total_files = files.len();

        for (file_idx, file) in files.iter().enumerate() {
            let file_path = file.path();

            println!(
                "   -- {:?} ({} of {})",
                file_path,
                file_idx + 1,
                total_files
            );

            // Run the test for each file (if it is a file)
            if file_path.is_file() {
                let _ = run_test_for_file(
                    file_path.to_str().unwrap(),
                    DEFAULT_COMPANY_TOKEN_PROCESSOR_CONFIG,
                );
            }
        }
    }
}

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/test_utils/constants.rs
-------------------------------------
// TODO: Use path lib
pub const TEST_FILES_DIRECTORY: &str = "tests/test_files";

// TODO: Use path lib
pub const TEST_SYMBOLS_CSV_PATH: &str = "../data/company_symbol_list.csv";

=====================================

File: /home/jeremy/Projects/ticker-sniffer/ticker_sniffer/test_utils/lib.rs
-------------------------------------
use csv::Reader;
use std::error::Error;
use std::{fs, path::Path};
use ticker_sniffer::{
    extract_tickers_from_text_with_custom_config, CompanySymbolList, CompanyTokenProcessorConfig,
    Error as LibError, TickerSymbol, TickerSymbolFrequencyMap,
};
// pub use models::EvaluationResult;
pub mod constants;
use constants::TEST_SYMBOLS_CSV_PATH;

/// Utility to load symbols from a CSV file for testing and benchmarking.
pub fn load_company_symbol_list_from_file(
    file_path: &str,
) -> Result<CompanySymbolList, Box<dyn Error>> {
    let mut company_symbols_list = CompanySymbolList::new();
    let mut reader = Reader::from_path(file_path)?;

    // Use headers to extract columns
    let headers = reader.headers()?.clone();

    for record in reader.records() {
        let record = record?;
        // Extract values based on header names
        let symbol = record.get(headers.iter().position(|h| h == "Symbol").unwrap());
        let company_name = record.get(headers.iter().position(|h| h == "Company Name").unwrap());
        let comma_separated_alternate_names =
            record.get(headers.iter().position(|h| h == "Alternate Names").unwrap());

        let alternate_names: Vec<String> = if let Some(names) = comma_separated_alternate_names {
            names
                .split(',')
                .map(|name| name.trim().to_string()) // Trim whitespace and convert to String
                .collect()
        } else {
            Vec::new() // Default to an empty vector if alternate names are missing
        };

        if let Some(symbol) = symbol {
            company_symbols_list.push((
                symbol.to_uppercase(),
                company_name.map(|name| name.to_string()),
                alternate_names,
            ));
        } else {
            eprintln!("Skipping invalid row: {:?}", record);
        }
    }

    Ok(company_symbols_list)
}

// Helper function to get the expected tickers from the text file
pub fn get_expected_tickers(file_path: &Path) -> Vec<TickerSymbol> {
    // Read the content of the text file
    let content = fs::read_to_string(file_path).expect("Failed to read test file");

    // Extract tickers from lines starting with EXPECTED:
    content
        .lines()
        .filter_map(|line| {
            let line = line.trim();
            if line.starts_with("EXPECTED:") {
                Some(line.replace("EXPECTED:", "").trim().to_string())
            } else {
                None
            }
        })
        .collect()
}

// Helper function to check if the file has an EXPECTED_FAILURE line
pub fn get_expected_failure(file_path: &Path) -> Option<TickerSymbol> {
    let content = fs::read_to_string(file_path).expect("Failed to read test file");

    content.lines().find_map(|line| {
        let line = line.trim();
        if line.starts_with("EXPECTED_FAILURE:") {
            Some(line.replace("EXPECTED_FAILURE:", "").trim().to_string())
        } else {
            None
        }
    })
}

// Helper function to run the test for each file in the directory
pub fn run_test_for_file(
    test_file_path: &str,
    company_token_processor_config: &CompanyTokenProcessorConfig,
) -> Result<
    (
        TickerSymbolFrequencyMap,
        Vec<TickerSymbol>,
        Vec<TickerSymbol>,
    ),
    LibError,
> {
    // Load symbols from a test CSV file
    let company_symbols_list = load_company_symbol_list_from_file(TEST_SYMBOLS_CSV_PATH)
        .expect("Failed to load symbols from CSV");

    // Read the content of the text file
    let raw_text = fs::read_to_string(test_file_path).expect("Failed to read test file");

    // Filter out lines starting with 'EXPECTED:', 'EXPECTED_FAILURE:', or 'COMMENT:'
    let filtered_text: String = raw_text
        .lines()
        .filter(|line| {
            !line.trim_start().starts_with("EXPECTED:")
                && !line.trim_start().starts_with("EXPECTED_FAILURE:")
                && !line.trim_start().starts_with("COMMENT:")
        })
        .collect::<Vec<&str>>()
        .join("\n");

    // Extract tickers from the filtered text
    let results_ticker_symbol_frequency_map = extract_tickers_from_text_with_custom_config(
        &company_token_processor_config,
        &filtered_text,
        &company_symbols_list,
    )?;

    // Get the expected tickers from the file
    let expected_tickers = get_expected_tickers(&Path::new(test_file_path));

    // Separate actual results into a vector of just tickers
    let actual_tickers: Vec<TickerSymbol> = results_ticker_symbol_frequency_map
        .iter()
        .map(|(symbol, _confidence)| symbol)
        .cloned()
        .collect();

    // Determine unexpected and missing tickers
    let unexpected_tickers: Vec<TickerSymbol> = actual_tickers
        .iter()
        .filter(|ticker| !expected_tickers.contains(ticker))
        .cloned()
        .collect();

    let missing_tickers: Vec<TickerSymbol> = expected_tickers
        .iter()
        .filter(|ticker| !actual_tickers.contains(ticker))
        .cloned()
        .collect();

    // Assertions for correctness
    assert_eq!(
        actual_tickers.len(),
        expected_tickers.len(),
        "{} - Expected {} tickers but found {}. Missing: {:?}, Unexpected: {:?}",
        test_file_path,
        expected_tickers.len(),
        actual_tickers.len(),
        missing_tickers,
        unexpected_tickers
    );

    for ticker in &expected_tickers {
        assert!(
            actual_tickers.contains(ticker),
            "{} - Expected ticker {:?} was not found in results. Found: {:?}",
            test_file_path,
            ticker,
            actual_tickers
        );
    }

    for ticker in &unexpected_tickers {
        assert!(
            !expected_tickers.contains(ticker),
            "{} - Unexpected ticker {:?} found in results.",
            test_file_path,
            ticker
        );
    }

    // Return the results along with the lists of unexpected and missing tickers
    Ok((
        results_ticker_symbol_frequency_map,
        unexpected_tickers,
        missing_tickers,
    ))
}

=====================================

